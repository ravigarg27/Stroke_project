{
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import cPickle as pickle\n",
      "from pprint import pprint\n",
      "from time import time\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from xgboost.sklearn import XGBClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "from skopt import gp_minimize\n",
      "from hyperopt import hp\n",
      "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
      "\n",
      "import xgboost as xgb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross validation and Hyperopt method "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def cross_validated_scorer(X_train_list, y_train_list, X_val_list, y_val_list, model_class, params, kfolds=5):\n",
      "    \n",
      "    print params\n",
      "    mod = model_class.set_params(**params)\n",
      "    scores = []\n",
      "    \n",
      "    t0 = time()\n",
      "    \n",
      "    for i in xrange(len(X_train_list)):\n",
      "        \n",
      "        train_text = X_train_list[i]\n",
      "        train_label = y_train_list[i]\n",
      "        \n",
      "        val_text = X_val_list[i]\n",
      "        val_label = y_val_list[i]\n",
      "        \n",
      "        mod.fit(train_text, train_label)\n",
      "        \n",
      "        val_score = mod.score(val_text, val_label)\n",
      "        scores.append(val_score)\n",
      "        \n",
      "    print \"Duration \", time() - t0\n",
      "    scores = np.array(scores)\n",
      "    print scores.mean()\n",
      "    cv_score = -1 * scores.mean()\n",
      "    return cv_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def hyperopt_search(X_train_list, y_train_list, X_val_list, y_val_list, model_class, param_grid, max_evals=100):\n",
      "\n",
      "    def objective(params):\n",
      "        err = cross_validated_scorer(\n",
      "            X_train_list, y_train_list, X_val_list, y_val_list, model_class, params)\n",
      "        return {'loss': err, 'params': params, 'status': STATUS_OK}\n",
      "    trials = Trials()\n",
      "    results = fmin(\n",
      "        objective, param_grid, algo=tpe.suggest,\n",
      "        trials=trials, max_evals=max_evals)\n",
      "    return trials.results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifiers to find hyperparameter for "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "nBClf = MultinomialNB()\n",
      "\n",
      "nBparameters = {\n",
      "    'alpha': hp.uniform('alpha', 0.0, 2.0)\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rfClf = RandomForestClassifier()\n",
      "\n",
      "RFparameters = {\n",
      "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
      "    'max_features': hp.choice('max_features', range(1,20)),\n",
      "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n",
      "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "svmClf = SVC()\n",
      "\n",
      "SVMparameters = {\n",
      "    'C': hp.uniform('C', 0, 10.0),\n",
      "    'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
      "    'gamma': hp.uniform('gamma', 0, 20.0)\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "xgbClf = XGBClassifier(objective='multi:softprob')\n",
      "\n",
      "XGBparameters = {\n",
      "    'max_depth' : hp.choice('max_depth', range(4, 13, 1)),\n",
      "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
      "    'n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),\n",
      "    'gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
      "    'min_child_weight' : hp.quniform('min_child_weight', 1, 5, 1),\n",
      "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),\n",
      "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"../../data/train_corpus_labels_full_reports.pkl\", \"rb\") as filein:\n",
      "    train_corpus = pickle.load(filein)\n",
      "    labels = pickle.load(filein)\n",
      "\n",
      "print train_corpus.shape\n",
      "print labels.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "preprocess = Pipeline([\n",
      "        ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "        ('feat_sel', SelectFromModel(LogisticRegression(penalty='l1', C=0.1)))\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(n_components=150)\n",
      "sfmlr = SelectFromModel(LogisticRegression(penalty='l1', C=0.1))\n",
      "combined_features = FeatureUnion([(\"pca\", pca), (\"lr\", sfmlr)])\n",
      "preprocess2 = Pipeline([\n",
      "        ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "        ('ToDense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
      "        ('features', combined_features)\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_text_list = []\n",
      "train_label_list = []\n",
      "val_text_list = []\n",
      "val_label_list = []\n",
      "\n",
      "k_fold = StratifiedKFold(5)\n",
      "\n",
      "for train_indices, val_indices in k_fold.split(train_corpus, labels):\n",
      "    train_text = np.asarray(train_corpus[train_indices])\n",
      "    train_y    = np.asarray(labels[train_indices])\n",
      "\n",
      "    val_text  = np.asarray(train_corpus[val_indices])\n",
      "    val_y     = np.asarray(labels[val_indices])\n",
      "    \n",
      "    start = time()\n",
      "    preprocessor = preprocess2.fit(train_text, train_y)\n",
      "    \n",
      "    train_text_processed = preprocessor.transform(train_text)\n",
      "    print \"Train shape \", train_text_processed.shape\n",
      "    val_text_processed = preprocessor.transform(val_text)\n",
      "    print \"Val shape \", val_text_processed.shape\n",
      "    train_text_list.append(train_text_processed)\n",
      "    train_label_list.append(train_y)\n",
      "    val_text_list.append(val_text_processed)\n",
      "    val_label_list.append(val_y)\n",
      "    \n",
      "    print \"Duration \", time() - start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nbResults = hyperopt_search(train_text_list, train_label_list, val_text_list, val_label_list, nBClf, nBparameters)\n",
      "nb_df = pd.DataFrame(nbResults)\n",
      "nb_df.head()\n",
      "minLoss = nb_df.loss.min()\n",
      "print -1*minLoss\n",
      "nb_df[nb_df[\"loss\"] == minLoss]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svmResults = hyperopt_search(train_text_list, train_label_list, val_text_list, val_label_list, svmClf, SVMparameters)\n",
      "svm_df = pd.DataFrame(svmResults)\n",
      "svm_df.head()\n",
      "minLoss = svm_df.loss.min()\n",
      "print -1 * minLoss\n",
      "svm_df[svm_df[\"loss\"] == minLoss]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfResults = hyperopt_search(train_text_list, train_label_list, val_text_list, val_label_list, rfClf, RFparameters)\n",
      "rf_df = pd.DataFrame(rfResults)\n",
      "rf_df.head()\n",
      "minLoss = rf_df.loss.min()\n",
      "print -1 * minLoss\n",
      "rf_df[rf_df[\"loss\"] == minLoss]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xgbResults = hyperopt_search(train_text_list, train_label_list, val_text_list, val_label_list, xgbClf, XGBparameters)\n",
      "xgb_df = pd.DataFrame(xgbResults)\n",
      "xgb_df.head()\n",
      "minLoss = xgb_df.loss.min()\n",
      "print -1 * minLoss\n",
      "xgb_df[xgb_df[\"loss\"] == minLoss]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "'''\n",
      "\n",
      "### If finding hyperparameter for the whole pipeline can use the following code\n",
      "\n",
      "def cross_validated_scorer(X_train, y_train, model_class, params, kfolds=5):\n",
      "    \n",
      "    print params\n",
      "    mod = model_class.set_params(**params)\n",
      "    t0 = time()\n",
      "    scores = cross_val_score(\n",
      "        mod,\n",
      "        X_train,\n",
      "        y=y_train,\n",
      "        cv=kfolds,\n",
      "        n_jobs=-1)\n",
      "    print \"Duration \", time() - t0\n",
      "    print scores.mean()\n",
      "    cv_score = -1 * scores.mean()\n",
      "    return cv_score\n",
      "\n",
      "def hyperopt_search(X_train, y_train, model_class, param_grid, max_evals=100):\n",
      "\n",
      "    def objective(params):\n",
      "        err = cross_validated_scorer(\n",
      "            X_train, y_train, model_class, params)\n",
      "        return {'loss': err, 'params': params, 'status': STATUS_OK}\n",
      "    trials = Trials()\n",
      "    results = fmin(\n",
      "        objective, param_grid, algo=tpe.suggest,\n",
      "        trials=trials, max_evals=max_evals)\n",
      "    return trials.results\n",
      "\n",
      "nBpipeline = Pipeline([\n",
      "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "    ('feat_sel', SelectFromModel(LogisticRegression(penalty='l1', C=0.01))),\n",
      "    ('clf', MultinomialNB())\n",
      "])\n",
      "\n",
      "nBparameters = {\n",
      "    'clf__alpha': hp.uniform('alpha', 0.0, 2.0)\n",
      "}\n",
      "\n",
      "RFpipeline = Pipeline([\n",
      "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "    ('feat_sel', LogisticRegression(penalty='l1', C=0.01)),\n",
      "    ('clf', RandomForestClassifier())\n",
      "])\n",
      "\n",
      "RFparameters = {\n",
      "    'clf__max_depth': hp.choice('max_depth', range(1,20)),\n",
      "    'clf__max_features': hp.choice('max_features', range(1,5)),\n",
      "    'clf__n_estimators': hp.choice('n_estimators', range(1,20)),\n",
      "    'clf__criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
      "    'clf__scale': hp.choice('scale', [0, 1]),\n",
      "    'clf__normalize': hp.choice('normalize', [0, 1])\n",
      "}\n",
      "\n",
      "SVMpipeline = Pipeline([\n",
      "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "    ('feat_sel', LogisticRegression(penalty='l1', C=0.01)),\n",
      "    ('clf', SVC())\n",
      "])\n",
      "\n",
      "SVMparameters = {\n",
      "    'clf__C': hp.uniform('C', 0, 10.0),\n",
      "    'clf__kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
      "    'clf__gamma': hp.uniform('gamma', 0, 20.0)\n",
      "}\n",
      "\n",
      "XGBpipeline = Pipeline([\n",
      "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word', stop_words=\"english\", min_df=0.05, max_df=0.9)),\n",
      "    ('feat_sel', LogisticRegression(penalty='l1', C=0.01)),\n",
      "    ('clf', XGBClassifier(objective='multi:softprob'))\n",
      "])\n",
      "\n",
      "XGBparameters = {\n",
      "    'clf__max_depth' : hp.choice('max_depth', range(4, 13, 1)),\n",
      "    'clf__learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
      "    'clf__n_estimators' : hp.choice('n_estimators', range(20, 205, 5)),\n",
      "    'clf__gamma' : hp.quniform('gamma', 0, 0.50, 0.01),\n",
      "    'clf__min_child_weight' : hp.quniform('min_child_weight', 1, 5, 1),\n",
      "    'clf__subsample' : hp.quniform('subsample', 0.1, 1, 0.01),\n",
      "    'clf__colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)\n",
      "}\n",
      "\n",
      "\"\"\"SVMresults = hyperopt_search(train_corpus, labels, SVMpipeline, SVMparameters)\n",
      "RFresults = hyperopt_search(train_corpus, labels, RFpipeline, SVMparameters)\n",
      "XGBresults = hyperopt_search(train_corpus, labels, XGBpipeline, XGBparameters)\n",
      "NBresults = hyperopt_search(train_corpus, labels, nBpipeline, nBparameters)\"\"\"\n",
      "\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import cPickle as pickle\n",
      "from pprint import pprint\n",
      "from time import time\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "from scipy import interp\n",
      "from itertools import cycle\n",
      "\n",
      "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.metrics import classification_report, accuracy_score\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_selection import SelectFromModel\n",
      "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
      "from sklearn.preprocessing import label_binarize\n",
      "from mlxtend.plotting import plot_decision_regions\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "\n",
      "import xgboost as xgb\n",
      "from xgboost.sklearn import XGBClassifier\n",
      "import seaborn as sns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cross-validation with pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def crossValidation(n_folds, pipeline, X, y):\n",
      "    k_fold = StratifiedKFold(n_folds)\n",
      "    scores = []\n",
      "    for train_indices, test_indices in k_fold.split(X, y):\n",
      "        train_text = np.asarray(X[train_indices])\n",
      "        train_y    = np.asarray(y[train_indices])\n",
      "\n",
      "        test_text  = np.asarray(X[test_indices])\n",
      "        test_y = np.asarray(y[test_indices])\n",
      "        test_y_binarize = label_binarize(test_y, classes=[0,1,2,3,4])\n",
      "        \n",
      "        pipeline.fit(train_text, train_y)\n",
      "        vocab = pipeline.named_steps[\"vect\"].vocabulary_\n",
      "        #selected_features = np.sum(pipeline.named_steps[\"f_select\"].get_support())\n",
      "        \n",
      "        \n",
      "        print \"Number of training samples \", train_text.shape[0]\n",
      "        print \"Number of validation samples \", test_text.shape[0]\n",
      "        print \"Number of features before feature selection \", len(vocab)\n",
      "        #print \"Number of features after feature selection \", selected_features\n",
      "        print \"Sample distribution in train \", np.bincount(train_y)\n",
      "        print \"Class distribution in train \", np.bincount(train_y) / float(np.sum(np.bincount(train_y)))\n",
      "        print \"Sample distribution in test \", np.bincount(test_y)\n",
      "        print \"Class distribution in test \", np.bincount(test_y) / float(np.sum(np.bincount(test_y)))\n",
      "        \n",
      "        train_score = pipeline.score(train_text, train_y)\n",
      "        val_score = pipeline.score(test_text, test_y)\n",
      "        print \"Training and validation score \", train_score, val_score\n",
      "        \n",
      "        pred_y = pipeline.predict(test_text)\n",
      "        print \"Samples predicted in each class \", np.bincount(pred_y)\n",
      "        report = classification_report(test_y, pred_y)\n",
      "        conf_matrix = confusion_matrix(test_y, pred_y)\n",
      "        featImp = feature_importance(pipeline.named_steps[\"clf\"])\n",
      "        print conf_matrix\n",
      "        print report\n",
      "        \n",
      "\n",
      "        pred_y_prob = pipeline.predict_proba(test_text)\n",
      "        \n",
      "        plot_roc_curves(test_y_binarize, pred_y_prob)\n",
      "        \n",
      "        scores.append(val_score)\n",
      "        break\n",
      "        \n",
      "    score = sum(scores) / len(scores)\n",
      "    return score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grid search with pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_confusion_matrix(cm, classes,\n",
      "                          normalize=False,\n",
      "                          title='Confusion matrix',\n",
      "                          cmap=plt.cm.Blues):\n",
      "\n",
      "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
      "    plt.title(title)\n",
      "    plt.colorbar()\n",
      "    tick_marks = np.arange(len(classes))\n",
      "    plt.xticks(tick_marks, classes, rotation=45)\n",
      "    plt.yticks(tick_marks, classes)\n",
      "\n",
      "    if normalize:\n",
      "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
      "        print(\"Normalized confusion matrix\")\n",
      "    else:\n",
      "        print('Confusion matrix, without normalization')\n",
      "\n",
      "    print(cm)\n",
      "\n",
      "    thresh = cm.max() / 2.\n",
      "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
      "        plt.text(j, i, cm[i, j],\n",
      "                 horizontalalignment=\"center\",\n",
      "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.ylabel('True label')\n",
      "    plt.xlabel('Predicted label')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_roc_curves(y_test, y_score):\n",
      "    \n",
      "    n_classes = y_test.shape[1]\n",
      "    lw = 2\n",
      "    \n",
      "    fpr = dict()\n",
      "    tpr = dict()\n",
      "    roc_auc = dict()\n",
      "    \n",
      "    print y_test.shape\n",
      "    print y_score.shape\n",
      "    \n",
      "    for i in range(n_classes):\n",
      "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
      "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
      "\n",
      "    # Compute micro-average ROC curve and ROC area\n",
      "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
      "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
      "    print \"Micro Auc \",auc(fpr[\"micro\"], tpr[\"micro\"])\n",
      "    \n",
      "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
      "\n",
      "    # Then interpolate all ROC curves at this points\n",
      "    mean_tpr = np.zeros_like(all_fpr)\n",
      "    for i in range(n_classes):\n",
      "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
      "\n",
      "    # Finally average it and compute AUC\n",
      "    mean_tpr /= n_classes\n",
      "\n",
      "    fpr[\"macro\"] = all_fpr\n",
      "    tpr[\"macro\"] = mean_tpr\n",
      "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
      "    print \"Macro auc \",auc(fpr[\"macro\"], tpr[\"macro\"])\n",
      "    \n",
      "    # Plot all ROC curves\n",
      "    plt.figure()\n",
      "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
      "             label='micro-average ROC curve (area = {0:0.2f})'\n",
      "                   ''.format(roc_auc[\"micro\"]),\n",
      "             color='deeppink', linestyle=':', linewidth=4)\n",
      "\n",
      "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
      "             label='macro-average ROC curve (area = {0:0.2f})'\n",
      "                   ''.format(roc_auc[\"macro\"]),\n",
      "             color='navy', linestyle=':', linewidth=4)\n",
      "\n",
      "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'black'])\n",
      "    for i, color in zip(range(n_classes), colors):\n",
      "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
      "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
      "                 ''.format(i, roc_auc[i]))\n",
      "\n",
      "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.05])\n",
      "    plt.xlabel('False Positive Rate')\n",
      "    plt.ylabel('True Positive Rate')\n",
      "    plt.title('ROC analysis')\n",
      "    plt.legend(loc=\"lower right\")\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def feature_importance(xgbClf):\n",
      "    \n",
      "    featImp = xgbClf.booster().get_fscore()\n",
      "    print featImp\n",
      "    \n",
      "    return featImp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Various pipelines "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pipeline 1 : Unigram features, Count weights, SGD classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfIDFVect = TfidfVectorizer(ngram_range=(1,3), smooth_idf=1, use_idf=1, min_df=0.05, max_df=0.9, analyzer='word', sublinear_tf=1, stop_words=\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "countVect = CountVectorizer(ngram_range=(1,3), min_df=0.05, max_df=0.9, analyzer='word', stop_words=\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifiers for feature selection "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(n_components=150)\n",
      "sfmlr = SelectFromModel(LogisticRegression(penalty='l1', C=0.01))\n",
      "combined_features = FeatureUnion([(\"pca\", pca), (\"lr\", sfmlr)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classifiers for prediction "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "svmClassifier = SVC(probability=True)\n",
      "naiveBayesClassifier = MultinomialNB()\n",
      "rfClassifier = RandomForestClassifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "xgb1 = XGBClassifier(\n",
      "        learning_rate = 0.1,\n",
      "        n_estimators=100,\n",
      "        max_depth=5,\n",
      "        min_child_weight=5,\n",
      "        gamma=0.3,\n",
      "        subsample=0.8,\n",
      "        colsample_bytree=0.8,\n",
      "        objective= 'multi:softmax',\n",
      "        nthread=-1,\n",
      "        scale_pos_weight=1,\n",
      "        seed=27)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Count based pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Logistic Regression based feature selection\n",
      "count_LR_SVMpipeline = Pipeline([\n",
      "        ('vect', countVect),\n",
      "        ('ToDense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
      "        ('f_select', lr),\n",
      "        ('clf', svmClassifier)\n",
      "    ])\n",
      "\n",
      "count_LR_NBpipeline = Pipeline([\n",
      "        ('vect', countVect),\n",
      "        ('f_select', lr),\n",
      "        ('clf', naiveBayesClassifier)\n",
      "    ])\n",
      "\n",
      "count_LR_RFpipeline = Pipeline([\n",
      "        ('vect', countVect),\n",
      "        ('f_select', lr),\n",
      "        ('clf', rfClassifier)\n",
      "    ])\n",
      "\n",
      "count_LR_XGBpipeline = Pipeline([\n",
      "    ('vect', countVect),\n",
      "    ('ToDense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
      "    ('features', combined_features),\n",
      "    ('clf', xgb1),\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Load data "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"../../data/train_corpus_labels_full_reports.pkl\", \"rb\") as filein:\n",
      "    train_corpus = pickle.load(filein)\n",
      "    labels = pickle.load(filein)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print train_corpus.shape\n",
      "print labels.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_LR_NBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_LR_SVMpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_LR_RFpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_LR_XGBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_RF_NBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_RF_RFpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "crossValidation(5, count_RF_SVMpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_RF_XGBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_SVM_NBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_SVM_RFpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "crossValidation(5, count_SVM_SVMpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossValidation(5, count_SVM_XGBpipeline, train_corpus, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "df_results = pd.read_excel(\"../../prelim_results.xlsx\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results.groupby(\"Classifier\").mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_results.groupby(\"Classifier\").std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.barplot(x='Classifier', y='Accuracy', data=df_results)\n",
      "plt.title(\"Accuracy across classifiers\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.barplot(x='Classifier', y='Micro ROC', data=df_results)\n",
      "plt.title(\"Micro ROC across classifiers\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.barplot(x='Classifier', y='Macro ROC', data=df_results)\n",
      "plt.title(\"Macro ROC across classifiers\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}